# -*- coding: utf-8 -*-
"""STAGE - All Tickers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QduNDGQxz2dEgwnF7UQ9woddNhV_P3Jb

# INITIALIZATION

## Imports
"""

import requests
import pandas as pd
import json

from time import sleep
from datetime import datetime

import os, shutil
from google.colab import drive
drive.mount('/content/drive')

"""## API Parameters"""

api_key = "IdcyAL8kOdog9S4oRwYgUJp6QnepEOET"
api_url_string = "&apiKey="+api_key

api_timeout_minutes = 5

tgt_dir_hist = "/content/drive/MyDrive/Projects/01 - Data Staging/House & Senate Stock Tracking/All Tickers/historical/"
tgt_dir_tost = "/content/drive/MyDrive/Projects/01 - Data Staging/House & Senate Stock Tracking/All Tickers/top-of-stack/"

date_string = datetime.today().strftime('%Y-%m-%d')

"""## Delete File in Top of Stack Folder"""

# DELETE ALL FILES IN THE TOP-OF-STACK DIRECTORY

for filename in os.listdir(tgt_dir_tost):
    file_path = os.path.join(tgt_dir_tost, filename)
    try:
        if os.path.isfile(file_path) or os.path.islink(file_path):
            os.unlink(file_path)
        elif os.path.isdir(file_path):
            shutil.rmtree(file_path)
    except Exception as e:
        print('Failed to delete %s. Reason: %s' % (file_path, e))

"""# FUNCTIONS"""

def Get_Next_Results(all_tickers,url):
  response = requests.get(url)
  response_dict = response.json()
  while response_dict['status'] != "ERROR":
    all_tickers = all_tickers.append(pd.DataFrame(response_dict['results']))
    try:
      url = response_dict['next_url'] + api_url_string
    except KeyError:
      url = "END"
      break
    response = requests.get(url)
    response_dict = response.json()
  return all_tickers, url

"""# SCRIPT"""

# INITIAL VALUES

all_tickers = pd.DataFrame()
url = "https://api.polygon.io/v3/reference/tickers?active=true&limit=1000"+api_url_string
counter = 0

while url != "END":
  [all_tickers, url] = Get_Next_Results(all_tickers,url)
  print(counter)
  counter += 1
  if url == "END":
    break
  sleep(api_timeout_minutes*60)

all_tickers = all_tickers.reindex()
all_tickers = all_tickers.drop_duplicates(subset=["ticker"])

all_tickers.to_csv(tgt_dir_hist + "all_tickers_" + date_string + ".csv",index=False)
all_tickers.to_csv(tgt_dir_tost + "all_tickers_" + date_string + ".csv",index=False)